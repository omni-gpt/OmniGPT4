data:
  class_path: omnigpt4.data.MMChat
  init_args:
    datasets:
      - urls: data/llava_instruct_150k/{000000..000006}.tar
        sample_rate: 1
    batch_size: 4
    num_workers: 8
    max_length: 512
    chat_prompt_manager:
      system_message:
        - A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.###\n
      human_name: Human
      assistant_name: Assistant
      conversation_template: "{human_name}: {human_text}###\n{assistant_name}: {assistant_text}###\n"
      tokenizer_name_or_path: bigscience/bloomz-7b1
      image_processor:
        class_path: omnigpt4.prompts.ImageProcessor
        init_args:
          crop_size: 224
          min_scale: 0.5
          max_scale: 1.0
      num_tokens_per_image: 32

model:
  class_path: omnigpt4.pl_modules.omnigpt4.OmniGPT4
  init_args:
    visual_model_name_or_path: Salesforce/blip2-flan-t5-xxl
    language_model_name_or_path: bigscience/bloomz-7b1
    language_projection_weight_path: weights/blip2_bloomz_7b1_stage_1_kt01xa66_00040000.safetensors
    compile_visual_model: False
    compile_qformer: False
    lora_config:
      r: 16
      lora_alpha: 16
      lora_dropout: 0.1
      inference_mode: False
    cache_dir: .cache
    optimizer_config:
      init_lr: 2e-5
      min_lr: 1e-6
      betas: [0.9, 0.999]
      weight_decay: 0.0001
      norm_weight_decay: 0.0
      num_warmup_steps: 100
      warmup_init_lr: 1e-6

trainer:
  devices: auto
  strategy: deepspeed_stage_2
  max_steps: 4_000
  precision: 16-mixed

  accumulate_grad_batches: 4

  logger:
    - class_path: omnigpt4.utils.logger.WandbLogger
      init_args:
        project: OmniGPT4
        entity: sotalab
        name: blip2_bloomz_7b1_stage_2_lora

  callbacks:
    - class_path: TQDMProgressBar
      init_args:
        refresh_rate: 50
    - class_path: ModelCheckpoint
      init_args:
        filename: "ckpt_{step:08d}"
        auto_insert_metric_name: False
        save_top_k: -1
        every_n_train_steps: 1_000
    - LearningRateMonitor

  default_root_dir: wandb
  num_sanity_val_steps: 0

seed_everything: 2333
